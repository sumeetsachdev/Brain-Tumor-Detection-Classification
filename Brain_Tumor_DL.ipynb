{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Utg6uNbDNSbK"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "!unzip \"/content/drive/My Drive/datasets/archive.zip\" -d \"/content\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U27bwLEoNecT"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
        "from tensorflow.keras.applications.densenet import DenseNet201, preprocess_input\n",
        "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, BatchNormalization, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "id7eJH42OnIg"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 150\n",
        "\n",
        "labels = [\"glioma_tumor\",\"meningioma_tumor\",\"no_tumor\",\"pituitary_tumor\"]\n",
        "\n",
        "print('\\nTrain Data')\n",
        "\n",
        "data_train, y_detect_train, y_class_train = [], [], []\n",
        "\n",
        "training_path = \"/content/Training\"\n",
        "\n",
        "for label in labels:\n",
        "  print(label)\n",
        "  path = os.path.join(training_path, label)\n",
        "  for img in tqdm(os.listdir(path)):\n",
        "    image = cv2.imread(os.path.join(path, img))\n",
        "    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    data_train.append(image)\n",
        "    y_class_train.append(label)\n",
        "    tmp = 0 if label == \"no_tumor\" else 1\n",
        "    y_detect_train.append(tmp)\n",
        "X_train = np.array(data_train)\n",
        "\n",
        "print('\\nTest Data')\n",
        "\n",
        "data_test, y_detect_test, y_class_test = [], [], []\n",
        "\n",
        "testing_path = \"/content/Testing\"\n",
        "\n",
        "for label in labels:\n",
        "  print(label)\n",
        "  path = os.path.join(testing_path, label)\n",
        "  for img in tqdm(os.listdir(path)):\n",
        "    image = cv2.imread(os.path.join(path, img))\n",
        "    #print(img)\n",
        "    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    data_test.append(image)\n",
        "    y_class_test.append(label)\n",
        "    tmp = 0 if label == 'no_tumor' else 1\n",
        "    y_detect_test.append(tmp)\n",
        "X_test = np.array(data_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAClHn_nO6bt"
      },
      "outputs": [],
      "source": [
        "#X_train, y_class_train, y_detect_train = shuffle(X_train,y_class_train, y_detect_train, random_state=101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZjcLEX7SR8e"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le1 = LabelEncoder()\n",
        "le2 = LabelEncoder()\n",
        "le3 = LabelEncoder()\n",
        "le4 = LabelEncoder()\n",
        "\n",
        "y_class_train = le1.fit_transform(y_class_train)\n",
        "y_class_test = le2.fit_transform(y_class_test)\n",
        "y_detect_train = le3.fit_transform(y_detect_train)\n",
        "y_detect_test = le4.fit_transform(y_detect_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B5txytYhnYL"
      },
      "outputs": [],
      "source": [
        "TRAINING_DIR = training_path\n",
        "TEST_DIR = testing_path\n",
        "\n",
        "seed = 10\n",
        "\n",
        "im_shape = (250, 250)\n",
        "\n",
        "BATCH_SIZE = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAE1Unw8hjul"
      },
      "outputs": [],
      "source": [
        "data_generator = ImageDataGenerator(rescale=1./255, validation_split=0.3)\n",
        "val_data_generator = ImageDataGenerator(rescale=1./255, validation_split=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJAobNtthgS4"
      },
      "outputs": [],
      "source": [
        "data_generator = ImageDataGenerator(\n",
        "        validation_split=0.3,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        #color_mode=grayscale,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')\n",
        "\n",
        "val_data_generator = ImageDataGenerator(rescale=1./255, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ediwgqQ7heaj"
      },
      "outputs": [],
      "source": [
        "train_generator = data_generator.flow_from_directory(TRAINING_DIR, target_size=im_shape, shuffle=True, seed=seed,\n",
        "                                                     class_mode='categorical', batch_size=BATCH_SIZE, subset=\"training\")\n",
        "# Generator para parte validação\n",
        "validation_generator = val_data_generator.flow_from_directory(TRAINING_DIR, target_size=im_shape, shuffle=False, seed=seed,\n",
        "                                                     class_mode='categorical', batch_size=BATCH_SIZE, subset=\"validation\")\n",
        "\n",
        "# Generator para dataset de teste\n",
        "test_generator = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_generator.flow_from_directory(TEST_DIR, target_size=im_shape, shuffle=False, seed=seed,\n",
        "                                                     class_mode='categorical', batch_size=BATCH_SIZE)\n",
        "\n",
        "nb_train_samples = train_generator.samples\n",
        "nb_validation_samples = validation_generator.samples\n",
        "nb_test_samples = test_generator.samples\n",
        "classes = list(train_generator.class_indices.keys())\n",
        "print('Classes: '+str(classes))\n",
        "num_classes  = len(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NY6f_S_taERv"
      },
      "outputs": [],
      "source": [
        "#base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(im_shape[0], im_shape[1], 3))\n",
        "\n",
        "#base_model = DenseNet201(weights='imagenet', include_top=False, input_shape=(im_shape[0], im_shape[1], 3))\n",
        "\n",
        "#base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(im_shape[0], im_shape[1], 3))\n",
        "\n",
        "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(im_shape[0], im_shape[1], 3))\n",
        "\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(300, activation='relu')(x)\n",
        "predictions = Dense(num_classes, activation='softmax', kernel_initializer='random_uniform')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freezing pretrained layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable=True\n",
        "    \n",
        "optimizer = Adam()\n",
        "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf1swxhPb1xP",
        "outputId": "f3ee5742-b30d-4764-f892-d2a58c01b7ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            " 82/125 [==================>...........] - ETA: 31:24 - loss: 1.9576 - accuracy: 0.2953"
          ]
        }
      ],
      "source": [
        "epochs = 50\n",
        "\n",
        "# Saving the best model\n",
        "callbacks_list = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath='model.h5',\n",
        "        monitor='val_loss', save_best_only=True, verbose=1),\n",
        "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,verbose=1)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=nb_train_samples // BATCH_SIZE,\n",
        "        epochs=epochs,\n",
        "        callbacks = callbacks_list,\n",
        "        validation_data=validation_generator,\n",
        "        verbose = 1,\n",
        "        validation_steps=nb_validation_samples // BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHbQI1I_erc4"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIwkl-r9ex4t"
      },
      "outputs": [],
      "source": [
        "tensorboard = TensorBoard(log_dir = 'logs')\n",
        "checkpoint = ModelCheckpoint(\"effnet.h5\",monitor=\"val_accuracy\",save_best_only=True,mode=\"auto\",verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.3, patience = 2, min_delta = 0.001,\n",
        "                              mode='auto',verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNkpmp85cPkv"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train,y_class_train,validation_split=0.1, epochs =70, verbose=1, batch_size=32,\n",
        "                   callbacks=[tensorboard,checkpoint,reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ_uNVBFcZiJ"
      },
      "outputs": [],
      "source": [
        "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
        "loss_function=nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "best_accuracy=0.0\n",
        "\n",
        "for epoch in range(20):\n",
        "    \n",
        "    #Evaluation and training on training dataset\n",
        "    model.train()\n",
        "    train_accuracy=0.0\n",
        "    train_loss=0.0\n",
        "    \n",
        "    for i, (images,labels) in enumerate(train_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            images=Variable(images.cuda())\n",
        "            labels=Variable(labels.cuda())\n",
        "            \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs=model(images)\n",
        "        loss=loss_function(outputs,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "        train_loss+= loss.cpu().data*images.size(0)\n",
        "        _,prediction=torch.max(outputs.data,1)\n",
        "        \n",
        "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
        "        \n",
        "    train_accuracy=train_accuracy/train_count\n",
        "    train_loss=train_loss/train_count\n",
        "    \n",
        "    \n",
        "    # Evaluation on testing dataset\n",
        "    model.eval()\n",
        "    \n",
        "    test_accuracy=0.0\n",
        "    for i, (images,labels) in enumerate(test_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            images=Variable(images.cuda())\n",
        "            labels=Variable(labels.cuda())\n",
        "            \n",
        "        outputs=model(images)\n",
        "        _,prediction=torch.max(outputs.data,1)\n",
        "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
        "    \n",
        "    test_accuracy=test_accuracy/test_count\n",
        "    \n",
        "    \n",
        "    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n",
        "    \n",
        "    if test_accuracy>best_accuracy:\n",
        "        torch.save(model.state_dict(),'best_checkpoint.model')\n",
        "        best_accuracy=test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxP-CRBpcial"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SML_Project_DL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}